{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from docling_parse.docling_parse import pdf_parser_v2\n",
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./docs/sub_cat/Meta Llama Responsible Use Guide.pdf', './docs/sub_cat/Unit_Cooler.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Already parsed : Meta Llama Responsible Use Guide\n",
      ">>>>> Already parsed : Unit_Cooler\n",
      ">>>>> All Parsings are Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_level_name(path:str) -> list:  # 폴더 구조(lv1, lv2, lv3를 metadata로 추출하는 함수)\n",
    "    temp = path.split(\"/\") \n",
    "    lv1 = temp[1]\n",
    "    if temp[2]:\n",
    "        if temp[2] != temp[-1]:\n",
    "            lv2 = temp[2]\n",
    "            lv3 = temp[-1].replace(\".pdf\", \"\")\n",
    "        else:\n",
    "            lv2 = None\n",
    "            lv3 = temp[-1].replace(\".pdf\", \"\")\n",
    "    result = [lv1, lv2, lv3]\n",
    "    return result\n",
    "\n",
    "def main_filepath_extractor(path:str, total_results=[]) -> list:   # 폴더 트리를 리커시브하게 읽어서 전체 PDF 파일의 full 경로를 리스트에 수집\n",
    "    all_items = os.listdir(path)\n",
    "    files = [f for f in all_items if os.path.isfile(os.path.join(path, f))]\n",
    "    results = [os.path.join(path, file) for file in files]\n",
    "    results = [result.replace(\"\\\\\", \"/\") for result in results]\n",
    "    total_results.extend(results)\n",
    "    dirs = [f for f in all_items if os.path.isdir(os.path.join(path, f))]\n",
    "    if dirs:\n",
    "        dirs = [path+\"/\" + lv2_dir for lv2_dir in dirs]\n",
    "        for dir in dirs:\n",
    "            main_filepath_extractor(dir)\n",
    "    return total_results\n",
    "\n",
    "def docling_parser(path:str):\n",
    "    try:\n",
    "        print(f\">>> Parsing Start: {path}\")\n",
    "        parser = pdf_parser_v2()\n",
    "        doc_key = f\"key={path}\"\n",
    "        success = parser.load_document(doc_key, path)\n",
    "        num_pages = parser.number_of_pages(doc_key)\n",
    "        print(f\"------ Total Page Number : {num_pages} ----------\")\n",
    "\n",
    "        converter = DocumentConverter()\n",
    "        loaded_docs = converter.convert(path)\n",
    "        print(\"------ Doc Loading is Completed ----------\")\n",
    "\n",
    "        lv1, lv2, filename = extract_level_name(path)\n",
    "        results = []\n",
    "        for page_number in range(num_pages-1):\n",
    "            docling_text = loaded_docs.document.export_to_markdown(page_no=int(page_number)+1)\n",
    "            lang_doc = Document(page_content=docling_text, metadata={\"Page\": str(page_number), \"First Division\": str(lv1), \"Second Division\": str(lv2), \"File Name\": str(filename), \"File Path\": str(path)})   \n",
    "                     \n",
    "            results.append(lang_doc)\n",
    "            with open(f'./parsed_docs/parsed_{filename}.pkl', 'ab') as file:\n",
    "                pickle.dump(results, file)\n",
    "            \n",
    "        print(f\"------- Done (length of results: {len(results)}------------\")\n",
    "\n",
    "        with open(f'./parsed_docs/parsed_{filename}.pkl', 'wb') as file:\n",
    "            pickle.dump(results, file)\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error - {e}\")\n",
    "        with open(f'./parsed_docs/error_{filename}.pkl', 'wb') as file:\n",
    "            pickle.dump(results, file)\n",
    "\n",
    "def main(lv1_path:str):\n",
    "    total_files = main_filepath_extractor(lv1_path)\n",
    "    print(total_files)\n",
    "\n",
    "    picklefiles = os.listdir('./parsed_docs')\n",
    "    picklefiles = [i.replace(\".pkl\", \"\") for i in picklefiles]\n",
    "    picklefiles = [i.replace(\"parsed_\", \"\") for i in picklefiles]\n",
    "\n",
    "    for file_path in tqdm(total_files):\n",
    "        filename = file_path.split(\"/\")[-1]\n",
    "        filename = filename.replace(\".pdf\", \"\")\n",
    "\n",
    "        if filename not in picklefiles:   # 기완료 중복 체크\n",
    "            print(f\">>>>> Do Parsing :  {filename}\")\n",
    "            docling_parser(path=file_path)\n",
    "        else: print(f\">>>>> Already parsed : {filename}\")\n",
    "    \n",
    "    print(\">>>>> All Parsings are Completed\")\n",
    "\n",
    "lv1_path = \"./docs\"\n",
    "main(lv1_path=lv1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"Meta Llama Responsible Use Guide\"\n",
    "# filename = \"Unit_Cooler\"\n",
    "with open(f'./parsed_docs/parsed_{filename}.pkl', 'rb') as file:\n",
    "    parsed_text = pickle.load(file)\n",
    "\n",
    "len(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Page': '4', 'First Division': 'docs', 'Second Division': 'sub_cat', 'File Name': 'Meta Llama Responsible Use Guide', 'File Path': './docs/sub_cat/Meta Llama Responsible Use Guide.pdf'}\n",
      "<!-- image -->\n",
      "\n",
      "## How to use this guide\n",
      "\n",
      "This guide is a resource for developers that outlines common approaches to building responsibly at each level of an LLM-powered product. It covers best practices and considerations that developers should evaluate in the context of their specific use case and market. It also highlights some mitigation strategies and resources available to developers to address risks at various points in the system. These best practices should be considered holistically because strategies adopted at one level can impact the entire system.\n",
      "\n",
      "The recommendations included in this guide reflect current research on responsible generative AI. We expect these to evolve as the field advances and access to foundation models grows, inviting further innovation on AI safety. Decisions to implement best practices should be evaluated based on the jurisdiction where your products will be deployed and should follow your company's internal legal and risk management processes.\n"
     ]
    }
   ],
   "source": [
    "page_num = 4\n",
    "print(parsed_text[page_num].metadata)\n",
    "print(parsed_text[page_num].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
